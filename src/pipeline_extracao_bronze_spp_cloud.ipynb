{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a798d6-7799-4dee-b7a6-06be4308bd5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 tenacity\n",
    "%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38db49d0-c16e-480c-b592-8d5b5c3fc904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c66bb23-cfad-4851-a32c-a4f063e57af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Para convers√£o Pandas <-> Spark mais r√°pida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a12992-6792-4e01-b11d-b73c28a3dcfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IngestaoSSPIncrementalMensal:\n",
    "    def __init__(self, spark_session: SparkSession, url_base: str, padrao_caminho_excel: str, ano_inicial: int, nome_tabela: str):\n",
    "        self.spark = spark_session\n",
    "        self.url_base = url_base.rstrip('/')\n",
    "        self.padrao_caminho_excel = padrao_caminho_excel.lstrip('/')\n",
    "        self.ano_inicial = ano_inicial\n",
    "        self.nome_tabela = nome_tabela\n",
    "        self.anos_encontrados = []\n",
    "        self.session = self.criar_session_com_retries()\n",
    "\n",
    "    def criar_session_com_retries(self):\n",
    "        session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def descobrir_anos(self):\n",
    "        print(f\"üîé [INFO] Descobrindo anos dispon√≠veis a partir de {self.ano_inicial}...\")\n",
    "        ano = self.ano_inicial\n",
    "        while True:\n",
    "            url_teste = f\"{self.url_base}/{self.padrao_caminho_excel.format(ano)}\"\n",
    "            try:\n",
    "                resp = self.session.head(url_teste, timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    print(f\"   üü¢ [ENCONTRADO] {url_teste}\")\n",
    "                    self.anos_encontrados.append((ano, url_teste))\n",
    "                    ano += 1\n",
    "                else:\n",
    "                    print(f\"   üî¥ [NAO EXISTE] {url_teste}, parando.\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"   üî¥ [ERRO] {e}\")\n",
    "                break\n",
    "        print(f\"‚úÖ [INFO] Total de anos encontrados: {len(self.anos_encontrados)}\")\n",
    "\n",
    "    def baixar_e_converter_para_spark(self, ano: int, url_xlsx: str):\n",
    "        print(f\"‚¨áÔ∏è [INFO] Baixando e convertendo arquivo do ano {ano}: {url_xlsx}\")\n",
    "        try:\n",
    "            resp = self.session.get(url_xlsx, timeout=600)\n",
    "            resp.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"   üî¥ [ERRO] Falha ao baixar arquivo: {e}\")\n",
    "            return None\n",
    "\n",
    "        df_pandas = pd.read_excel(BytesIO(resp.content))\n",
    "        df_pandas = df_pandas.astype(str)\n",
    "\n",
    "        df_spark = self.spark.createDataFrame(df_pandas)\n",
    "\n",
    "        # Renomear ANO_BO para ano se existir\n",
    "        colunas_upper = [c.upper() for c in df_spark.columns]\n",
    "        if \"ANO_BO\" in colunas_upper:\n",
    "            for c in df_spark.columns:\n",
    "                if c.upper() == \"ANO_BO\":\n",
    "                    df_spark = df_spark.withColumnRenamed(c, \"ano\")\n",
    "                    break\n",
    "        else:\n",
    "            df_spark = df_spark.withColumn(\"ano\", F.lit(str(ano)))\n",
    "\n",
    "        # For√ßa explicitamente todas as colunas (inclusive \"ano\") para string\n",
    "        for col in df_spark.columns:\n",
    "            df_spark = df_spark.withColumn(col, F.col(col).cast(\"string\"))\n",
    "\n",
    "        print(f\"‚úÖ [INFO] Ano {ano} baixado e convertido (coluna ano como string).\")\n",
    "        return df_spark\n",
    "\n",
    "    def unir_dataframes_dinamicamente(self, lista_dfs):\n",
    "        print(\"üõ†Ô∏è [INFO] Alinhando e unindo DataFrames dinamicamente...\")\n",
    "        colunas_totais = list(set(c for df in lista_dfs for c in df.columns))\n",
    "\n",
    "        dfs_alinhados = []\n",
    "        for df in lista_dfs:\n",
    "            colunas_faltantes = [col for col in colunas_totais if col not in df.columns]\n",
    "            for col in colunas_faltantes:\n",
    "                df = df.withColumn(col, F.lit(None).cast(\"string\"))\n",
    "\n",
    "            # For√ßar explicitamente todas as colunas (inclusive \"ano\") para string\n",
    "            for col in colunas_totais:\n",
    "                df = df.withColumn(col, F.col(col).cast(\"string\"))\n",
    "\n",
    "            df = df.select(colunas_totais)\n",
    "            dfs_alinhados.append(df)\n",
    "\n",
    "        df_final = dfs_alinhados[0]\n",
    "        for df in dfs_alinhados[1:]:\n",
    "            df_final = df_final.unionByName(df)\n",
    "\n",
    "        print(\"‚úÖ [INFO] DataFrames unidos com sucesso.\")\n",
    "        return df_final\n",
    "\n",
    "\n",
    "    def salvar_com_sobrescrita_parcial(self, df_novo, ano: int):\n",
    "        print(f\"üíæ [INFO] Salvando ano {ano} na tabela {self.nome_tabela}...\")\n",
    "        try:\n",
    "            tabelas = self.spark.sql(f\"SHOW TABLES LIKE '{self.nome_tabela}'\").collect()\n",
    "            if not tabelas:\n",
    "                df_novo.write.format(\"delta\").mode(\"overwrite\").saveAsTable(self.nome_tabela)\n",
    "                print(\"‚úÖ [INFO] Tabela criada com sucesso.\")\n",
    "                return\n",
    "\n",
    "            df_existente = self.spark.table(self.nome_tabela)\n",
    "            # Ajuste expl√≠cito: converte coluna 'ano' existente para string\n",
    "            df_existente = df_existente.withColumn(\"ano\", F.col(\"ano\").cast(\"string\"))\n",
    "            \n",
    "            df_filtrado = df_existente.filter(F.col(\"ano\") != str(ano))\n",
    "            df_union = self.unir_dataframes_dinamicamente([df_filtrado, df_novo])\n",
    "\n",
    "            df_union.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(self.nome_tabela)\n",
    "            print(f\"‚úÖ [INFO] Ano {ano} sobrescrito com sucesso.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   üî¥ [ERRO] Ao salvar a tabela: {e}\")\n",
    "\n",
    "    def run_ingerir_ano_a_ano(self):\n",
    "        from datetime import datetime\n",
    "        ano_corrente = datetime.now().year\n",
    "\n",
    "        print(\"üöÄ [INFO] Iniciando fluxo incremental mensal.\")\n",
    "        self.descobrir_anos()\n",
    "\n",
    "        for ano, url_xlsx in self.anos_encontrados:\n",
    "            df_spark = self.baixar_e_converter_para_spark(ano, url_xlsx)\n",
    "            if df_spark is None:\n",
    "                continue\n",
    "\n",
    "            if ano < ano_corrente:\n",
    "                self.salvar_com_sobrescrita_parcial(df_spark, ano)\n",
    "            elif ano == ano_corrente:\n",
    "                self.salvar_com_sobrescrita_parcial(df_spark, ano)\n",
    "\n",
    "        print(\"üèÅ [INFO] Processo de ingest√£o conclu√≠do com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e133b2-a0c0-480e-b9a3-8315c5f781b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dbw_prd_bra_01;\n",
    "USE SCHEMA ssp_cloud;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18f3fd9-5508-4178-a506-6c52977c1dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url_base = \"https://www.ssp.sp.gov.br\"\n",
    "padrao_excel = \"assets/estatistica/transparencia/spDados/SPDadosCriminais_{}.xlsx\"\n",
    "ano_inicial = 2022\n",
    "nome_tabela = \"bronze_sp_dados_criminais\"  # Apenas o nome da tabela\n",
    "\n",
    "ingestor = IngestaoSSPIncrementalMensal(\n",
    "    spark_session=spark,\n",
    "    url_base=url_base,\n",
    "    padrao_caminho_excel=padrao_excel,\n",
    "    ano_inicial=ano_inicial,\n",
    "    nome_tabela=nome_tabela\n",
    ")\n",
    "\n",
    "ingestor.run_ingerir_ano_a_ano()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 12567073888013,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_extracao_bronze_spp_cloud",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
